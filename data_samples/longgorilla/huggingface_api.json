{
    "context": [
        "<api>AutoModel.from_pretrained('YituTech/conv-bert-base')</api> domain: Natural Language Processing Feature Extraction, framework: Hugging Face Transformers, functionality: Feature Extraction, api_name: YituTech/conv-bert-base, api_arguments: N/A, python_environment_requirements: transformers, example_code: N/A, performance: dataset: N/A, accuracy: N/A, description: A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library.",
        "<api>joblib.load(cached_download(hf_hub_url('julien-c/wine-quality', 'winequality-red.csv')))</api> domain: Tabular Tabular Classification, framework: Scikit-learn, functionality: Wine Quality classification, api_name: julien-c/wine-quality, api_arguments: [X], python_environment_requirements: [huggingface_hub, joblib, pandas], example_code: from huggingface_hub import hf_hub_url, cached_download\\nimport joblib\\nimport pandas as pd\\nREPO_ID = julien-c/wine-quality\\nFILENAME = sklearn_model.joblib\\nmodel = joblib.load(cached_download(\\n hf_hub_url(REPO_ID, FILENAME)\\n))\\ndata_file = cached_download(\\n hf_hub_url(REPO_ID, winequality-red.csv)\\n)\\nwinedf = pd.read_csv(data_file, sep=;)\\nX = winedf.drop([quality], axis=1)\\nY = winedf[quality]\\nprint(X[:3])\\nlabels = model.predict(X[:3])\\nmodel.score(X, Y), performance: dataset: julien-c/wine-quality, accuracy: 0.6616635397123202, description: A Simple Example of Scikit-learn Pipeline for Wine Quality classification. Inspired by https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976 by Saptashwa Bhattacharyya.",
        "<api>pipeline('text-classification', model='prithivida/parrot_adequacy_model')</api> domain: Natural Language Processing Text Classification, framework: Hugging Face Transformers, functionality: Transformers, api_name: prithivida/parrot_adequacy_model, api_arguments: , python_environment_requirements: transformers, example_code: , performance: dataset: , accuracy: , description: Parrot is a paraphrase-based utterance augmentation framework purpose-built to accelerate training NLU models. This model is an ancillary model for Parrot paraphraser.",
        "<api>DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')</api> domain: Natural Language Processing Text Classification, framework: Transformers, functionality: Text Classification, api_name: distilbert-base-uncased-finetuned-sst-2-english, api_arguments: [inputs], python_environment_requirements: [torch, transformers], example_code: \"import torch\\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\\ntokenizer = DistilBertTokenizer.from_pretrained(distilbert-base-uncased)\\nmodel = DistilBertForSequenceClassification.from_pretrained(distilbert-base-uncased-finetuned-sst-2-english)\\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\\nwith torch.no_grad():\\n logits = model(**inputs).logits\\npredicted_class_id = logits.argmax().item()\\nmodel.config.id2label[predicted_class_id]\", performance: dataset: glue, accuracy: 0.911, description: This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. It reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7). This model can be used for topic classification.",
        "<api>M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_418M')</api> domain: Natural Language Processing Text2Text Generation, framework: Hugging Face Transformers, functionality: Multilingual Translation, api_name: facebook/m2m100_418M, api_arguments: encoded_input: Encoded input text, target_lang: Target language code, python_environment_requirements: [transformers, sentencepiece], example_code: [from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer, hi_text = , chinese_text = , model = M2M100ForConditionalGeneration.from_pretrained(facebook/m2m100_418M), tokenizer = M2M100Tokenizer.from_pretrained(facebook/m2m100_418M), tokenizer.src_lang = hi, encoded_hi = tokenizer(hi_text, return_tensors=pt), generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(fr)), tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)], performance: dataset: WMT, accuracy: Not provided, description: M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation. It can directly translate between the 9,900 directions of 100 languages. To translate into a target language, the target language id is forced as the first generated token.",
        "<api>Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')</api> domain: Multimodal Image-to-Text, framework: Hugging Face Transformers, functionality: Transformers, api_name: blip2-flan-t5-xxl, api_arguments: raw_image: Image, question: Text, python_environment_requirements: [requests, PIL, transformers], example_code: \"import requests\\nfrom PIL import Image\\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\\nimg_url = https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg\\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert(RGB)\\nquestion = how many dogs are in the picture?\\ninputs = processor(raw_image, question, return_tensors=pt)\\nout = model.generate(**inputs)\\nprint(processor.decode(out[0], skip_special_tokens=True))\", performance: dataset: LAION, accuracy: Not provided, description: BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.",
        "<api>YOLO('keremberke/yolov8n-table-extraction')</api> domain: Computer Vision Object Detection, framework: Hugging Face Transformers, functionality: Table Extraction, api_name: keremberke/yolov8n-table-extraction, api_arguments: conf: 0.25, iou: 0.45, agnostic_nms: False, max_det: 1000, python_environment_requirements: [ultralyticsplus==0.0.23, ultralytics==8.0.21], example_code: [from ultralyticsplus import YOLO, render_result, \"model = YOLO(keremberke/yolov8n-table-extraction)\", \"model.overrides[conf] = 0.25\", \"model.overrides[iou] = 0.45\", \"model.overrides[agnostic_nms] = False\", \"model.overrides[max_det] = 1000\", \"image = https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\", results = model.predict(image), print(results[0].boxes), render = render_result(model=model, image=image, result=results[0]), render.show()], performance: dataset: table-extraction, accuracy: 0.967, description: \"An object detection model for extracting tables from documents. Supports two label types: bordered and borderless.\"",
        "<api>DiffusionPipeline.from_pretrained('google/ncsnpp-celebahq-256')</api> domain: Computer Vision Unconditional Image Generation, framework: Hugging Face Transformers, functionality: Unconditional Image Generation, api_name: google/ncsnpp-celebahq-256, api_arguments: model_id: google/ncsnpp-celebahq-256, python_environment_requirements: [diffusers], example_code: !pip install diffusers\\nfrom diffusers import DiffusionPipeline\\nmodel_id = google/ncsnpp-celebahq-256\\nsde_ve = DiffusionPipeline.from_pretrained(model_id)\\nimage = sde_ve()[sample]\\nimage[0].save(sde_ve_generated_image.png), performance: dataset: CIFAR-10, accuracy: Inception_score: 9.89, FID: 2.2, likelihood: 2.99, description: Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.",
        "<api>translation_pipeline('translation_de_to_en', model='Helsinki-NLP/opus-mt-de-en')</api> domain: Natural Language Processing Translation, framework: Hugging Face Transformers, functionality: Translation, api_name: opus-mt-de-en, api_arguments: [source languages: de, target languages: en], python_environment_requirements: [transformers], example_code: , performance: dataset: opus, accuracy: newssyscomb2009.de.en: 29.4, news-test2008.de.en: 27.8, newstest2009.de.en: 26.8, newstest2010.de.en: 30.2, newstest2011.de.en: 27.4, newstest2012.de.en: 29.1, newstest2013.de.en: 32.1, newstest2014-deen.de.en: 34.0, newstest2015-ende.de.en: 34.2, newstest2016-ende.de.en: 40.4, newstest2017-ende.de.en: 35.7, newstest2018-ende.de.en: 43.7, newstest2019-deen.de.en: 40.1, Tatoeba.de.en: 55.4, description: A German to English translation model trained on the OPUS dataset using the Hugging Face Transformers library.",
        "<api>load_model(cached_download(hf_hub_url('danupurnomo/dummy-titanic', 'titanic_model.h5')))</api> domain: Tabular Tabular Classification, framework: Scikit-learn, functionality: Binary Classification, api_name: danupurnomo/dummy-titanic, api_arguments: [new_data], python_environment_requirements: [huggingface_hub, joblib, pandas, numpy, tensorflow], example_code: \"from huggingface_hub import hf_hub_url, cached_download\\nimport joblib\\nimport pandas as pd\\nimport numpy as np\\nfrom tensorflow.keras.models import load_model\\nREPO_ID = danupurnomo/dummy-titanic\\nPIPELINE_FILENAME = final_pipeline.pkl\\nTF_FILENAME = titanic_model.h5\\nmodel_pipeline = joblib.load(cached_download(\\n hf_hub_url(REPO_ID, PIPELINE_FILENAME)\\n))\\nmodel_seq = load_model(cached_download(\\n hf_hub_url(REPO_ID, TF_FILENAME)\\n))\", performance: dataset: Titanic, accuracy: Not provided, description: This model is a binary classifier for predicting whether a passenger on the Titanic survived or not, based on features such as passenger class, age, sex, fare, and more.",
        "<api>AutoModelForCausalLM.from_pretrained('facebook/opt-66b')</api> domain: Natural Language Processing Text Generation, framework: Hugging Face Transformers, functionality: Text Generation, api_name: facebook/opt-66b, api_arguments: [input_ids, do_sample, num_return_sequences, max_length], python_environment_requirements: [transformers, torch], example_code: from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\\nimport torch\\nmodel = AutoModelForCausalLM.from_pretrained(facebook/opt-66b, torch_dtype=torch.float16).cuda()\\ntokenizer = AutoTokenizer.from_pretrained(facebook/opt-66b, use_fast=False)\\nprompt = Hello, I am conscious and\\ninput_ids = tokenizer(prompt, return_tensors=pt).input_ids.cuda()\\nset_seed(32)\\ngenerated_ids = model.generate(input_ids, do_sample=True, num_return_sequences=5, max_length=10)\\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True), performance: dataset: GPT-3, accuracy: roughly matched, description: OPT (Open Pre-trained Transformer) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, designed to enable reproducible and responsible research at scale. OPT models are trained to roughly match the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data collection and efficient training. The pretrained-only model can be used for prompting for evaluation of downstream tasks as well as text generation.",
        "<api>AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')</api> domain: Natural Language Processing Text Classification, framework: Hugging Face Transformers, functionality: financial-sentiment-analysis, api_name: ProsusAI/finbert, api_arguments: text, python_environment_requirements: transformers, example_code: \"from transformers import pipeline; classifier = pipeline(sentiment-analysis, model=ProsusAI/finbert); classifier(your_text_here)\", performance: dataset: Financial PhraseBank, accuracy: Not provided, description: FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning.",
        "<api>AutoModel.from_pretrained('Helsinki-NLP/opus-mt-sv-en').</api> domain: Natural Language Processing Translation, framework: Hugging Face Transformers, functionality: Translation, api_name: opus-mt-sv-en, api_arguments: [inputs], python_environment_requirements: [transformers], example_code: , performance: dataset: Tatoeba.sv.en, accuracy: BLEU: 64.5, chr-F: 0.763, description: A Swedish to English translation model trained on the OPUS dataset using the transformer-align architecture. The model is pre-processed with normalization and SentencePiece.",
        "<api>pipeline('visual-question-answering', model='GuanacoVQA').</api> domain: Multimodal Visual Question Answering, framework: Hugging Face, functionality: Visual Question Answering, api_name: JosephusCheung/GuanacoVQA, api_arguments: N/A, python_environment_requirements: transformers, torch, example_code: N/A, performance: dataset: JosephusCheung/GuanacoVQADataset, accuracy: N/A, description: A multilingual Visual Question Answering model supporting English, Chinese, Japanese, and German languages. It requires the combined use of the Guanaco 7B LLM model and is based on the implementation of MiniGPT-4."
    ],
    "location": "pre",
    "question": "Instruction: Write an API implementation that takes customer reviews as input and extracts features to analyze customer sentiment.",
    "answer": "AutoModel.from_pretrained('YituTech/conv-bert-base')"
}